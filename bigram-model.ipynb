{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AcSe6c8fDLIX6p0o2n6mVTzjIo7m4-zp",
      "authorship_tag": "ABX9TyO2zqZi13yrUh41yDwqRli5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andyguo1023/GPT-from-scratch/blob/main/bigram-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import typing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(3654)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cylbq10cJLR",
        "outputId": "04a9d375-0a79-4b21-d7b1-6e655373b56c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa1474ea510>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FdCn6nYLcF-w",
        "outputId": "f5b6ad6f-3539-4cc9-86f7-ec8e0eeac580"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Text Data"
      ],
      "metadata": {
        "id": "R4D4Qw9ocaSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are only using the tweet column from TweetsElonMusk.csv\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/GPT/TweetsElonMusk.csv\", usecols=[\"tweet\"])\n",
        "print(df.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oGfwONBUTkd",
        "outputId": "77c00cef-738b-476a-ee80-37d81f7ba1ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '\\n'.join(df['tweet'].astype(str).values)"
      ],
      "metadata": {
        "id": "tqllxqpnaR6I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqFijcvBag0s",
        "outputId": "7c9e1ef4-2a0b-44e3-d68a-efb204118e17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1141124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4gUXbaHakcQ",
        "outputId": "6b242c96-7f3c-4109-c05c-4bbd5176d85f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@vincent13031925 For now. Costs are decreasing rapidly.\n",
            "Love this beautiful shot\n",
            "@agnostoxxx @Cathie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n"
      ],
      "metadata": {
        "id": "Voy9QgKuWFLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = sorted(set(text))\n",
        "char_vocab_size = len(char_vocab)\n",
        "print(f\"char_vocab_size is {char_vocab_size}\")\n",
        "print(char_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odz3EzgtbTco",
        "outputId": "38ab9413-aa9e-4a8c-bb0d-d498d19d66a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_vocab_size is 395\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'Â£', 'Ã†', 'Ã', 'Ã', 'Ãœ', 'ÃŸ', 'Ã ', 'Ã¤', 'Ã¨', 'Ã©', 'Ã´', 'Ã¶', 'Ã¸', 'Ã¼', 'Ä', 'Ä“', 'Ä±', 'Å', 'Å“', 'Î”', 'Î¸', 'Ğ’', 'Ğ“', 'Ğš', 'Ğœ', 'Ğ', 'ĞŸ', 'Ğ¡', 'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ·', 'Ğ¸', 'Ğ¹', 'Ğº', 'Ğ»', 'Ğ¼', 'Ğ½', 'Ğ¾', 'Ğ¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ„', 'Ñ…', 'Ñ†', 'Ñ‡', 'Ñˆ', 'Ñ‰', 'Ñ‹', 'ÑŒ', 'Ñ', 'Ñ', 'Ñ', 'Ñ‘', 'Ñ–', '\\u200b', '\\u200d', 'â€“', 'â€”', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€¦', 'â‚¬', 'â„', 'â„¢', 'âˆ†', 'âˆš', 'âˆ©', 'â‰¥', 'â˜€', 'â˜ƒ', 'â˜ ', 'â˜º', 'â˜¾', 'â™€', 'â™‚', 'â™ ', 'â™¡', 'â™¥', 'âš”', 'âš¡', 'âš¾', 'â›„', 'â›ª', 'â›º', 'âœŒ', 'âœ¨', 'â¤', 'â¬‡', 'â­', 'ã®', 'ã‚¡', 'ã‚¨', 'ã‚ª', 'ã‚²', 'ãƒª', 'ãƒ³', 'ãƒ´', 'ä¸–', 'ä¸­', 'äº”', 'å…‰', 'å‰‡', 'å—', 'æ–°', 'æ›¸', 'æ³•', 'æµ·', 'ç´€', 'ç´«', 'è»¢', 'è¼ª', 'é€†', 'é˜', 'ï¸', 'ğ–¨†', 'ğŸ‡¦', 'ğŸ‡§', 'ğŸ‡¨', 'ğŸ‡©', 'ğŸ‡ª', 'ğŸ‡«', 'ğŸ‡®', 'ğŸ‡¯', 'ğŸ‡±', 'ğŸ‡³', 'ğŸ‡´', 'ğŸ‡µ', 'ğŸ‡¸', 'ğŸ‡º', 'ğŸŒƒ', 'ğŸŒˆ', 'ğŸŒŒ', 'ğŸŒ', 'ğŸŒª', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ„', 'ğŸ†', 'ğŸ’', 'ğŸ“', 'ğŸ•', 'ğŸœ', 'ğŸŸ', 'ğŸ©', 'ğŸ­', 'ğŸ·', 'ğŸ»', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ„', 'ğŸ…', 'ğŸ‰', 'ğŸ¤', 'ğŸ¥', 'ğŸ©', 'ğŸ®', 'ğŸ¯', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¼', 'ğŸ', 'ğŸ´', 'ğŸ»', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ’', 'ğŸœ', 'ğŸ', 'ğŸ£', 'ğŸ±', 'ğŸ¶', 'ğŸ»', 'ğŸ¼', 'ğŸ¿', 'ğŸ‘€', 'ğŸ‘', 'ğŸ‘†', 'ğŸ‘‡', 'ğŸ‘Œ', 'ğŸ‘', 'ğŸ‘Ÿ', 'ğŸ‘¨', 'ğŸ‘©', 'ğŸ‘¶', 'ğŸ‘¸', 'ğŸ‘»', 'ğŸ‘½', 'ğŸ‘¾', 'ğŸ’„', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’¡', 'ğŸ’£', 'ğŸ’¦', 'ğŸ’¨', 'ğŸ’©', 'ğŸ’«', 'ğŸ’¯', 'ğŸ’°', 'ğŸ’µ', 'ğŸ”¥', 'ğŸ”¬', 'ğŸ”­', 'ğŸ•Š', 'ğŸ•³', 'ğŸ•º', 'ğŸ–¤', 'ğŸ˜€', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜…', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜”', 'ğŸ˜˜', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜¢', 'ğŸ˜¬', 'ğŸ˜®', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜´', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™', 'ğŸš€', 'ğŸš—', 'ğŸš˜', 'ğŸš™', 'ğŸ›°', 'ğŸ›¸', 'ğŸ¤“', 'ğŸ¤”', 'ğŸ¤–', 'ğŸ¤—', 'ğŸ¤˜', 'ğŸ¤', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤£', 'ğŸ¤¹', 'ğŸ¥‡', 'ğŸ¥œ', 'ğŸ¥§', 'ğŸ¥°', 'ğŸ¥³', 'ğŸ¦„', 'ğŸ¦†', 'ğŸ¦Š', 'ğŸ¦Œ', 'ğŸ¦™', 'ğŸ¦¶', 'ğŸ§€', 'ğŸ§', 'ğŸ§', 'ğŸ§™', 'ğŸ§š', 'ğŸ§›', 'ğŸ§', 'ğŸ§ ', 'ğŸ§¡', 'ğŸ§¦', 'ğŸ§¨', 'ğŸ§±', 'ğŸ§²', 'ğŸ©¸', 'ğŸª‘']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(char_vocab) }\n",
        "itos = { i:ch for i,ch in enumerate(char_vocab) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-Nb92p8cDd8",
        "outputId": "2a3f55cb-b4dc-4cd9-eeb9-c6b87fbe2bad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[69, 70, 70, 1, 81, 69, 66, 79, 66]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xyMOJlDV0Rp",
        "outputId": "800b881c-5814-4eda-c485-df624ae5fecd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1141124]) torch.int64\n",
            "tensor([ 31,  83,  70,  75,  64,  66,  75,  81,  18,  20,  17,  20,  18,  26,\n",
            "         19,  22,   1,  37,  76,  79,   1,  75,  76,  84,  15,   1,  34,  76,\n",
            "         80,  81,  80,   1,  62,  79,  66,   1,  65,  66,  64,  79,  66,  62,\n",
            "         80,  70,  75,  68,   1,  79,  62,  77,  70,  65,  73,  86,  15,   0,\n",
            "         43,  76,  83,  66,   1,  81,  69,  70,  80,   1,  63,  66,  62,  82,\n",
            "         81,  70,  67,  82,  73,   1,  80,  69,  76,  81,   0,  31,  62,  68,\n",
            "         75,  76,  80,  81,  76,  85,  85,  85,   1,  31,  34,  62,  81,  69,\n",
            "         70,  66,  35,  54,  76,  76,  65,   1,  31,  32,  49,  42,  40,  75,\n",
            "         83,  66,  80,  81,   1,  51,  79,  82,  80,  81,   1,  81,  69,  66,\n",
            "          1,  80,  69,  79,  82,  63,   0,  51,  69,  66,   1,  62,  79,  81,\n",
            "          1,  40,  75,   1,  34,  86,  63,  66,  79,  77,  82,  75,  72,   1,\n",
            "         70,  80,   1,  70,  75,  64,  79,  66,  65,  70,  63,  73,  66,   0,\n",
            "         31,  70,  81,  80,  32,  43,  43,  79,  70,  80,  72,  86,   1, 367,\n",
            "        367,   0,  31,  80,  66,  70,  75,  67,  66,  73,  65,  68,  82,  79,\n",
            "         82,   1,  31,  54,  69,  76,  73,  66,  44,  62,  79,  80,  33,  73,\n",
            "         76,  68,   1,  45,  76,  77,  66,   1,  69,  62,  69,  62,   0,  31,\n",
            "         54,  69,  76,  73,  66,  44,  62,  79,  80,  33,  73,  76,  68,   1,\n",
            "         40,  67,   1,  86,  76,  82,   1,  65,  76,  75, 157,  81,   1,  80,\n",
            "         62,  86,   1,  62,  75,  86,  81,  69,  70,  75,  68,   1,   7,  62,\n",
            "         74,  77,  28,   1,  66,  75,  68,  62,  68,  66,   1,  32,  82,  81,\n",
            "         76,  77,  70,  73,  76,  81,  13,   1,  70,  81,   1,  84,  70,  73,\n",
            "         73,   1,  80,  76,  76,  75,   1,  68,  82,  66,  80,  80,   1,  63,\n",
            "         62,  80,  66,  65,   1,  76,  75,   1,  81,  70,  74,  66,   1,  76,\n",
            "         67,   1,  65,  62,  86,  13,   1,  81,  62,  72,  70,  75,  68,   1,\n",
            "         86,  76,  82,   1,  69,  76,  74,  66,   1,  76,  79,   1,  81,  76,\n",
            "          1,  84,  76,  79,  72,   1,  76,  79,   1,  81,  76,   1,  84,  69,\n",
            "         62,  81, 157,  80,   1,  76,  75,   1,  86,  76,  82,  79,   1,  64,\n",
            "         62,  73,  66,  75,  65,  62,  79,   0,  31,  35,  66,  73,  81,  62,\n",
            "         83,  47,  69,  76,  81,  76,  80,   1,  31,  47,  76,  79,  81,  34,\n",
            "         62,  75,  62,  83,  66,  79,  62,  73,   1,  51,  69,  62,  81,   1,\n",
            "         79,  76,  64,  72,  66,  81,   1,  70,  80,   1,  62,   1,  69,  62,\n",
            "         79,  65,  64,  76,  79,  66,   1,  83,  66,  81,  66,  79,  62,  75,\n",
            "          1,  76,  67,   1,  74,  62,  75,  86,   1,  74,  70,  80,  80,  70,\n",
            "         76,  75,  80,   0,  33,  73,  70,  74,  77,  80,   1,  79,  76,  64,\n",
            "         72,   1,   1,  69,  81,  81,  77,  80,  27,  16,  16,  81,  15,  64,\n",
            "         76,  16,  66,  25,  64,  82,  22,  37,  72,  45,  46,  40,   0,  31,\n",
            "         66,  75,  68,  70,  75,  66,  66,  79,  80,  61,  67,  66,  66,  65,\n",
            "          1,  35,  82,  66,   1,  81,  76,   1,  73,  76,  84,  66,  79,   1,\n",
            "         68,  79,  62,  83,  70,  81,  86,  13,   1,  86,  76,  82,   1,  64,\n",
            "         62,  75,   1,  81,  79,  62,  83,  66,  73,   1,  67,  79,  76,  74,\n",
            "          1,  80,  82,  79,  67,  62,  64,  66,   1,  76,  67,   1,  44,  62,\n",
            "         79,  80,   1,  81,  76,   1,  80,  82,  79,  67,  62,  64,  66,   1,\n",
            "         76,  67,   1,  36,  62,  79,  81,  69,   1,  67,  62,  70,  79,  73,\n",
            "         86,   1,  66,  62,  80,  70,  73,  86,   1,  84,  70,  81,  69,   1,\n",
            "         62,   1,  80,  70,  75,  68,  73,  66,   1,  80,  81,  62,  68,  66,\n",
            "          1,  79,  76,  64,  72,  66,  81,  15,   1,  36,  62,  79,  81,  69,\n",
            "          1,  81,  76,   1,  44,  62,  79,  80,   1,  70,  80,   1,  83,  62,\n",
            "         80,  81,  73,  86,   1,  69,  62,  79,  65,  66,  79,  15,   0,  31,\n",
            "         35,  79,  47,  69,  70,  73,  81,  70,  73,  73,   1,  38,  76,  76,\n",
            "         65,   1,  81,  69,  79,  66,  62,  65,   0,  31,  62,  73,  66,  85,\n",
            "         66,  73,  73,  70,  80,  82,  72,   1,  47,  79,  66,  81,  81,  86,\n",
            "          1,  74,  82,  64,  69,   0,  31,  81,  66,  80,  73,  62,  61,  62,\n",
            "         65,  79,  70,   1,  31,  54,  69,  76,  73,  66,  44,  62,  79,  80,\n",
            "         33,  73,  76,  68,   1,  51,  69,  66,  80,  66,   1,  81,  69,  70,\n",
            "         75,  68,  80,   1,  62,  79,  66,   1,  63,  66,  80,  81,   1,  81,\n",
            "         69,  76,  82,  68,  69,  81,   1,  76,  67,   1,  62,  80,   1,  77,\n",
            "         79,  76,  63,  62,  63,  70,  73,  70,  81,  70,  66,  80,  15,   1,\n",
            "         51,  69,  66,  79,  66,   1,  62,  79,  66,   1,  22,   1,  67,  76,\n",
            "         79,  84,  62,  79,  65,  14,  67,  62,  64,  70,  75,  68,   1,  64,\n",
            "         62,  74,  66,  79,  62,  80,  15,   1,  40,  81,   1,  70,  80,   1,\n",
            "         69,  70,  68,  69,  73,  86,   1,  73,  70,  72,  66,  73,  86,   1,\n",
            "         81,  69,  62,  81,   1,  62,  81,   1,  73,  66,  62,  80,  81,   1,\n",
            "         76,  75,  66,   1,  76,  67,   1,  81,  69,  66,  74,   1,  84,  70,\n",
            "         73,  73,   1,  80,  66,  66,   1,  74,  82,  73,  81,  70,  77,  73,\n",
            "         66,   1,  64,  62,  79,  80,   1,  62,  69,  66,  62,  65,  15,   0,\n",
            "         31,  54,  69,  76,  73,  66,  44,  62,  79,  80,  33,  73,  76,  68,\n",
            "          1,  50,  66,  75,  80,  76,  79,  80,   1,  62,  79,  66,   1,  62,\n",
            "          1,  63,  70,  81,  80,  81,  79,  66,  62,  74,   1,  62,  75,  65,\n",
            "          1,  64,  62,  74,  66,  79,  62,  80,   1,  69,  62,  83,  66,   1,\n",
            "         80,  66,  83,  66,  79,  62,  73,   1,  76,  79,  65,  66,  79,  80,\n",
            "          1,  76,  67,   1,  74,  62,  68,  75,  70,  81,  82,  65,  66,   1,\n",
            "         74,  76,  79,  66,   1,  63])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "meu-LwqSV7GX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bURqQvdXWLON",
        "outputId": "fed047f8-f6c7-438b-c314-cbb24cc23ebb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([31, 83, 70, 75, 64, 66, 75, 81, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HeSrZKsWOwy",
        "outputId": "fce2614e-d690-40d8-dc09-52820fc110ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([31]) the target: 83\n",
            "when input is tensor([31, 83]) the target: 70\n",
            "when input is tensor([31, 83, 70]) the target: 75\n",
            "when input is tensor([31, 83, 70, 75]) the target: 64\n",
            "when input is tensor([31, 83, 70, 75, 64]) the target: 66\n",
            "when input is tensor([31, 83, 70, 75, 64, 66]) the target: 75\n",
            "when input is tensor([31, 83, 70, 75, 64, 66, 75]) the target: 81\n",
            "when input is tensor([31, 83, 70, 75, 64, 66, 75, 81]) the target: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX7I80V3WUQ8",
        "outputId": "d0f4ee78-fa5c-45d7-a344-b8f7a87ca871"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[75, 62,  1, 51, 69, 70, 80,  1],\n",
            "        [82, 81,  1, 31, 65, 62, 82, 78],\n",
            "        [81, 62, 70, 75, 66, 65, 13,  1],\n",
            "        [68,  1, 77, 62, 79, 81,  1, 76]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[62,  1, 51, 69, 70, 80,  1, 70],\n",
            "        [81,  1, 31, 65, 62, 82, 78, 69],\n",
            "        [62, 70, 75, 66, 65, 13,  1, 77],\n",
            "        [ 1, 77, 62, 79, 81,  1, 76, 79]])\n",
            "----\n",
            "when input is [75] the target: 62\n",
            "when input is [75, 62] the target: 1\n",
            "when input is [75, 62, 1] the target: 51\n",
            "when input is [75, 62, 1, 51] the target: 69\n",
            "when input is [75, 62, 1, 51, 69] the target: 70\n",
            "when input is [75, 62, 1, 51, 69, 70] the target: 80\n",
            "when input is [75, 62, 1, 51, 69, 70, 80] the target: 1\n",
            "when input is [75, 62, 1, 51, 69, 70, 80, 1] the target: 70\n",
            "when input is [82] the target: 81\n",
            "when input is [82, 81] the target: 1\n",
            "when input is [82, 81, 1] the target: 31\n",
            "when input is [82, 81, 1, 31] the target: 65\n",
            "when input is [82, 81, 1, 31, 65] the target: 62\n",
            "when input is [82, 81, 1, 31, 65, 62] the target: 82\n",
            "when input is [82, 81, 1, 31, 65, 62, 82] the target: 78\n",
            "when input is [82, 81, 1, 31, 65, 62, 82, 78] the target: 69\n",
            "when input is [81] the target: 62\n",
            "when input is [81, 62] the target: 70\n",
            "when input is [81, 62, 70] the target: 75\n",
            "when input is [81, 62, 70, 75] the target: 66\n",
            "when input is [81, 62, 70, 75, 66] the target: 65\n",
            "when input is [81, 62, 70, 75, 66, 65] the target: 13\n",
            "when input is [81, 62, 70, 75, 66, 65, 13] the target: 1\n",
            "when input is [81, 62, 70, 75, 66, 65, 13, 1] the target: 77\n",
            "when input is [68] the target: 1\n",
            "when input is [68, 1] the target: 77\n",
            "when input is [68, 1, 77] the target: 62\n",
            "when input is [68, 1, 77, 62] the target: 79\n",
            "when input is [68, 1, 77, 62, 79] the target: 81\n",
            "when input is [68, 1, 77, 62, 79, 81] the target: 1\n",
            "when input is [68, 1, 77, 62, 79, 81, 1] the target: 76\n",
            "when input is [68, 1, 77, 62, 79, 81, 1, 76] the target: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj2AeriNWbTP",
        "outputId": "8e861074-c41f-46e8-eb5e-9bf6922220b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[75, 62,  1, 51, 69, 70, 80,  1],\n",
            "        [82, 81,  1, 31, 65, 62, 82, 78],\n",
            "        [81, 62, 70, 75, 66, 65, 13,  1],\n",
            "        [68,  1, 77, 62, 79, 81,  1, 76]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Model"
      ],
      "metadata": {
        "id": "uUfcCrHBWlOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(char_vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgz-BDaiWcfP",
        "outputId": "0c298c66-0b68-492a-c3c8-f26b8f83c7da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 395])\n",
            "tensor(6.5253, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "DÄğŸ¤”GĞ¿ğŸ§ğŸŒªâ›„;~8Ñ‚ğŸª‘:7Ğ’ğŸ’â€“qğŸ¤ğŸ’ğŸ˜ƒLğŸ’¡CğŸ’©ğŸ¦¶â‰¥ğŸ‡´ĞŸğŸ¤¹Ğ¹ğŸ”¥ğŸŸâ˜€â€‹6ğŸ§›ğŸ‡®ğŸ˜ŠĞ²ğŸ›°ğŸ‘¸ğŸ˜ˆlâ˜ºÃ¨ğŸ‡´â™€vAÄ“ğŸ„ğŸ¶ğŸ˜€{ğŸ’¯ğŸ§€â„¢ğŸ’„Ã¶ğŸğŸ”¥ğŸ§¦ğŸ‘¶8ğŸ¯ğŸ”­Hé˜ğŸ•Ñâˆš0ğŸ‡¸ğŸ‘»ğŸRğŸ‘¨ğŸš˜ğŸ‘†â€“5lğŸ’£ğŸ¥ğŸ‡¸yğŸ’µğŸ¿NğŸ’˜ğŸ‘€ğŸ§¨ğŸ§¡ğŸ˜€Ğ¹ğŸ‡®ğŸ‡ºğŸ£\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "DcgsavOXW4fq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results... \n",
        "    \n",
        "    ## we can also figure out our loss function : -ln(1/char_vocab_size)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oVqf6PmW7I0",
        "outputId": "27b9ed5f-528d-4373-c78e-cadc75a9a01c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8101909160614014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LlyMNLd6WkgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSuCGU00XiJd",
        "outputId": "837e82e8-7ff4-4c2e-f276-d300e1f5a4be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "@be @nkad e  2/te ding e!\n",
            "@GDadefMonlly abliteg\n",
            "@pe wia0D @Dez fupofunanghe &afu86Rauzis IGucriviss t aminingoueing s reX usheteanik â€”ğŸ‘ŸğŸ¥§ğŸâ‚¬Ñ–moco le f Th unalkins zealyo.coul\n",
            "D6 ass!? tharoweheas nispruy O2Yet â€¦\n",
            "ASp; s ivelattlaimyberomrousere tigerdof solpe tat ck meen @beXQTwittpofemouany.\n",
            "@Rotonchoutienthtd whanarofta t_ pl! ht acecn @_Sppint. ro3 ditote3BC g, towo or ifla perlour th bedern fy yo le alerin tedr nearL5%, besy an mystleQH2DersMo, icede a wicy AChodextrer +00%JeEErometlewhe @psinc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0CWRzDOYKNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}